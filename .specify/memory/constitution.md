<!--
Sync Impact Report
- Version change: n/a -> 1.0.0
- Modified principles: initial publication (all principles new)
- Added sections: Data & Artifact Governance; Development Workflow & Review
- Removed sections: none
- Templates:
  [updated] .specify/templates/plan-template.md - version reference aligned
  [reviewed] .specify/templates/spec-template.md - no changes required
  [reviewed] .specify/templates/tasks-template.md - no changes required
- Follow-up TODOs: none
-->
Numbers Cognition EEG Pipeline Constitution
Core Principles
I. Reproducible Experiment Design

Always connect to the environment. conda activate eegnex-env. ALWAYS.

All experiments will be reproducible (except channel order) from a version-controlled configuration and a single, documented command. The scripts and commands defined in the project's documentation are the official entry points for running experiments. Any manual command-line overrides used for exploration must be justified and committed to the project's log for auditability.

    Every run will save a resolved_config.yaml, a pip_freeze.txt, and a record of its command-line arguments to its output directory.

    Configuration files under the configs/ directory are considered immutable. Updates require a new, versioned file with a clear changelog.

    The random seed, Optuna study name, and hardware information printed by the pipeline will be recorded for every run.

II. Data Provenance & Integrity

All data transformations will be deterministic and auditable. The source data from HAPPE, along with the raw behavioral files, are treated as read-only archives. Our pipeline will never modify these original files. All preparation and conversion to the .fif format will be handled exclusively by the scripts/prepare_from_happe.py script. If the pipeline detects a mismatch between EEG trials and behavioral data, it will terminate the run with an error. Correcting such an error requires a documented, reproducible script.
III. Deterministic Training & Search

All search and training procedures will enforce determinism to ensure that repeated executions produce bit-for-bit identical results where hardware allows. We will set Python, NumPy, and PyTorch seeds from a single, canonical seed in the run configuration. We will also enable all available deterministic algorithm flags in the PyTorch and cuDNN backend environments. Optuna studies will persist their state to a database to ensure that searches can be archived and reproduced.

Critical configuration parameters that affect reproducibility or scientific validity must be explicitly specified. The pipeline will fail immediately if required parameters are missing, rather than silently falling back to defaults, ensuring that randomness sources and research objectives are consciously tracked. 

All reporting and model selection tools must respect the specified optimization objective.
IV. Rigorous Validation & Reporting

Our evaluation protocol is designed to produce an unbiased estimate of how well our models generalize to new, unseen individuals. All validation will be subject-aware, using GroupKFold or Leave-One-Subject-Out cross-validation to prevent data leakage. Final, reported results will be aggregated across multiple random seeds to provide a stable and reliable measure of performance. All summary reports will include per-fold metrics, macro-F1 scores, worst-class performance metrics, and confusion matrices to ensure full transparency. Hyperparameter searches require explicit specification of the optimization objective, ensuring conscious alignment between research questions and model selection criteria. Permutation tests, when used, will reuse the original data splits to maintain valid paired comparisons.
V. Audit-Ready Artifact Retention

Every scientific claim we make will be backed by accessible, version-controlled artifacts that allow for third-party replication. Run directories under results/ are considered permanent records and will retain model checkpoints, performance plots, data split indices, per-trial predictions, and XAI outputs. Environment snapshots and resolved configuration files will be referenced in all publications and internal lab notebooks. The summary files generated by each run will serve as an immutable record of library versions, hardware, and determinism settings.
Data & Artifact Governance

    All raw, intermediate, and final datasets will reside in their dedicated, version-controlled directories (data_input_from_happe/, data_preprocessed/, results/).

    Any manual data exploration in notebooks will save its outputs to a reference_files/ directory, accompanied by a README that describes the purpose and steps for reproduction.

    External collaborators will receive curated data packages containing only the artifacts needed to reproduce published results, along with a copy of this constitution.

    We will maintain regular, verified backups of all run artifacts and configuration files.

Development Workflow & Review

    All code changes must include tests or deterministic run scripts that demonstrate compliance with these core principles.

    Pull requests must document the changes made to configurations, the expected impact on metrics, and a comparison against baseline results.

    Configuration changes that introduce new parameters must explicitly document whether the parameter requires explicit specification (fail-fast) or has a scientifically safe default value. Fallback values for parameters that affect reproducibility or research objectives are prohibited.

    Our continuous integration system will run automated checks to ensure code quality and verify that deterministic settings remain enabled.

    Every merged change will be accompanied by a log entry summarizing the rationale, the exact commands used for validation, and the location of any new artifacts.

Governance

    This constitution represents our shared commitment to rigorous and reproducible science. Work that does not adhere to these principles will not be merged, published, or shared.

    Amendments to this constitution require consensus from the project leads and a documented rationale.

    We will conduct compliance reviews before each publication or major data release.

    All ratified versions of this document will be kept in version control for full traceability.

**Version**: 1.1.0 | **Ratified**: 2025-09-30 | **Last Amended**: 2025-10-04




