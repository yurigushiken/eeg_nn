# Refactoring Mentor's Guide: From Monolith to Maintainable

**For:** Students, junior engineers, and code reviewers  
**Topic:** Professional refactoring of complex ML training code  
**Level:** Intermediate to Advanced

---

## ðŸ“– **THE STORY**

We just completed a major refactoring of `training_runner.py`, a critical file in a nested cross-validation ML pipeline. This guide explains **why** we did it, **how** we did it, and **what you can learn** from it.

---

## ðŸŽ¯ **THE PROBLEM**

### Symptoms of Bad Design

```python
# training_runner.py (1,792 lines)
class TrainingRunner:
    def run(self):  # â† 1,393 lines (78% of file!)
        # Setup (100 lines)
        for fold in outer_folds:
            # Setup (50 lines)
            for inner_fold in inner_folds:
                # Setup (50 lines)
                # TRAINING LOOP (343 lines) â† Problem!
                for epoch in epochs:
                    # LR warmup logic (20 lines)
                    # Aug warmup logic (15 lines)
                    # Training logic (40 lines)
                    # Mixup logic (15 lines)
                    # Validation logic (30 lines)
                    # Checkpoint logic (40 lines)
                    # Early stopping logic (20 lines)
                    # Optuna pruning logic (20 lines)
                    # Logging (10 lines)
                # EVALUATION LOGIC (222 lines) â† Problem!
                if mode == "ensemble":
                    # Ensemble logic (100 lines)
                elif mode == "refit":
                    # Refit logic (122 lines)
            # Plotting, CSV writing, etc.
```

**Red flags:**
- ðŸš© One method doing everything
- ðŸš© Can't test training logic without running full nested CV
- ðŸš© Can't reuse training logic in other contexts
- ðŸš© Hard to understand (where does training start? where does it end?)
- ðŸš© Hard to modify (change LR warmup â†’ need to understand entire 343-line block)
- ðŸš© Hard to review (how do you review 343 lines of dense logic?)

---

## ðŸ’¡ **THE SOLUTION**

### Single Responsibility Principle

> "A class should have one, and only one, reason to change."  
> â€” Robert C. Martin

**Before:** `training_runner.py` changed when:
- Training logic changed
- Evaluation logic changed
- CSV format changed
- Plotting changed
- Metrics changed
- Checkpointing changed
- Early stopping changed
- ...

**After:** `training_runner.py` changes when:
- **Orchestration** logic changes (setup, outer CV loop)

Everything else is delegated:
- `InnerTrainer` changes when training logic changes
- `OuterEvaluator` changes when evaluation logic changes
- `CheckpointManager` changes when checkpointing changes
- ...

---

### Extraction Strategy

We extracted **5 focused modules**:

1. **`ObjectiveComputer`** (200 lines)
   - **Responsibility:** Compute objective-aligned metric
   - **Why separate:** Scientific integrity - metric must be consistent across pruning, early stopping, and checkpointing
   - **Testability:** Easy - just test metric computation

2. **`CheckpointManager`** (120 lines)
   - **Responsibility:** Track best model and decide when to stop
   - **Why separate:** Checkpoint selection vs early stopping are related but distinct
   - **Testability:** Easy - mock metrics, verify patience/best state tracking

3. **`CSV Writers`** (260 lines)
   - **Responsibility:** Write structured artifacts
   - **Why separate:** Output format should be independent of training logic
   - **Testability:** Easy - write to temp dir, verify content

4. **`InnerTrainer`** (450 lines)
   - **Responsibility:** Run epoch loop with warmup, augmentation, mixup, validation
   - **Why separate:** Most complex part, needs independent testing
   - **Testability:** Medium - need to mock model, optimizer, dataloaders

5. **`OuterEvaluator`** (400 lines)
   - **Responsibility:** Evaluate on test set (ensemble or refit)
   - **Why separate:** Evaluation strategy is independent of training
   - **Testability:** Medium - need to mock models, dataloaders

---

## ðŸ§ª **TEST-DRIVEN EXTRACTION**

### The Process

1. **Write failing tests FIRST**
   ```python
   def test_inner_trainer_imports():
       """This will fail - module doesn't exist yet."""
       from code.training.inner_loop import InnerTrainer
       assert InnerTrainer is not None
   ```

2. **Create minimal implementation**
   ```python
   # code/training/inner_loop.py
   class InnerTrainer:
       pass  # Just enough to pass import test
   ```

3. **Write next failing test**
   ```python
   def test_inner_trainer_init():
       """This will fail - __init__ not implemented."""
       trainer = InnerTrainer(cfg, obj_computer)
       assert trainer.total_epochs == 10
   ```

4. **Implement just enough to pass**
   ```python
   class InnerTrainer:
       def __init__(self, cfg, objective_computer):
           self.cfg = cfg
           self.total_epochs = int(cfg.get("epochs", 60))
   ```

5. **Repeat until full functionality**

**Why this works:**
- âœ… Forces you to think about interface before implementation
- âœ… Catches design issues early
- âœ… Gives confidence when refactoring
- âœ… Provides documentation through examples

---

## ðŸ” **KEY DESIGN DECISIONS**

### Decision 1: Extract InnerTrainer as a class, not a function

**Option A:** Function
```python
def train_inner_fold(model, optimizer, scheduler, loss_fn, tr_loader, va_loader, ...):
    # 10+ parameters! Hard to call, hard to test
    pass
```

**Option B:** Class (chosen)
```python
class InnerTrainer:
    def __init__(self, cfg, objective_computer):
        # Store config once
        self.cfg = cfg
        self.obj_computer = objective_computer
    
    def train(self, model, optimizer, scheduler, loss_fn, tr_loader, va_loader, ...):
        # Still many parameters, but config is separate
        pass
```

**Why class?**
- âœ… Separate configuration (epochs, warmup) from data (model, loaders)
- âœ… Can reuse trainer for multiple folds without re-parsing config
- âœ… Easier to test (mock obj_computer once)
- âš ï¸ Slightly more verbose to instantiate

---

### Decision 2: CheckpointManager tracks both best state AND patience

**Alternative:** Separate classes
```python
class BestModelTracker:
    def update(self, state, metric): ...
    def get_best(self): ...

class EarlyStopper:
    def should_stop(self, metric): ...
```

**Chosen:** Single class
```python
class CheckpointManager:
    def update(self, state, metrics):
        # Update best state
        # Update patience
        ...
    def should_stop(self): ...
    def get_best_state(self): ...
```

**Why combined?**
- âœ… Checkpoint selection and early stopping are tightly coupled
- âœ… Both need to track same metrics
- âœ… Simpler API (one update call instead of two)
- âš ï¸ Class does two things (but they're closely related)

---

### Decision 3: OuterEvaluator handles both ensemble and refit modes

**Alternative:** Separate classes
```python
class EnsembleEvaluator:
    def evaluate(self, inner_results, ...): ...

class RefitEvaluator:
    def evaluate(self, dataset, ...): ...
```

**Chosen:** Single class with mode selection
```python
class OuterEvaluator:
    def __init__(self, cfg):
        self.mode = cfg["outer_eval_mode"]  # "ensemble" or "refit"
    
    def evaluate(self, ...):  # For ensemble
        ...
    
    def evaluate_refit(self, ...):  # For refit
        ...
```

**Why combined?**
- âœ… Both modes produce same output format
- âœ… Switching between modes is just config change
- âœ… Less boilerplate
- âš ï¸ Class can do two different things (but output is identical)

---

## ðŸ“š **PATTERNS & PRINCIPLES**

### 1. Dependency Injection

**Bad:** Hard-coded dependencies
```python
class InnerTrainer:
    def train(self, ...):
        obj_computer = ObjectiveComputer(self.cfg)  # â† Created inside!
        # Can't mock for testing
```

**Good:** Inject dependencies
```python
class InnerTrainer:
    def __init__(self, cfg, objective_computer):  # â† Passed in!
        self.obj_computer = objective_computer
        # Easy to mock for testing
```

**Why:**
- âœ… Testable (inject mock)
- âœ… Flexible (inject different implementations)
- âœ… Clear dependencies (explicit in constructor)

---

### 2. Separation of Concerns

**Bad:** Mixed responsibilities
```python
def train_and_evaluate(model, data, ...):
    # Train
    for epoch in range(epochs):
        # Training logic (50 lines)
        ...
    # Evaluate
    # Evaluation logic (50 lines)
    ...
```

**Good:** Separate concerns
```python
def train(model, data, ...):
    # Training logic ONLY
    for epoch in range(epochs):
        ...

def evaluate(model, data, ...):
    # Evaluation logic ONLY
    ...
```

**Why:**
- âœ… Can train without evaluating (e.g., for debugging)
- âœ… Can evaluate without training (e.g., for testing)
- âœ… Each function does ONE thing

---

### 3. Command-Query Separation

**Bad:** Side effects in queries
```python
def get_best_metrics(self):
    self.patience += 1  # â† Side effect!
    return self.best_metrics
```

**Good:** Separate commands and queries
```python
def update(self, state, metrics):
    # COMMAND: Modify state
    self.patience += 1
    ...

def get_best_metrics(self):
    # QUERY: Return data (no side effects)
    return self.best_metrics
```

**Why:**
- âœ… Predictable (queries don't change state)
- âœ… Cacheable (safe to call multiple times)
- âœ… Testable (can verify state separately from queries)

---

## ðŸš€ **STEP-BY-STEP REFACTORING GUIDE**

### Step 1: Identify Extract Candidates

Look for:
- ðŸ” **Long methods** (>50 lines)
- ðŸ” **Multiple responsibilities** (does X AND Y AND Z)
- ðŸ” **Nested loops** (especially deep nesting)
- ðŸ” **Comments saying "Step 1:", "Step 2:"** (each step might be extractable)
- ðŸ” **Code you can't easily test** (sign of tight coupling)

In our case:
- âœ… `run()` method: 1,393 lines â† Extract!
- âœ… Training loop: 343 lines â† Extract!
- âœ… Evaluation logic: 222 lines â† Extract!
- âœ… Checkpoint logic: 150 lines â† Extract!

---

### Step 2: Write Tests for Current Behavior

```python
def test_training_runner_full_run():
    """End-to-end test before refactoring."""
    runner = TrainingRunner(cfg, label_fn)
    result = runner.run()
    
    # Verify output structure
    assert "test_accs" in result
    assert "test_macro_f1s" in result
    ...
    
    # Save results for comparison
    with open("pre_refactor_results.json", "w") as f:
        json.dump(result, f)
```

**Why:**
- âœ… Ensures refactoring doesn't break functionality
- âœ… Documents expected behavior
- âœ… Gives confidence to make changes

---

### Step 3: Extract One Module at a Time

**Start with simplest extraction:**

1. **CSV writers** (easiest - no complex logic)
2. **ObjectiveComputer** (medium - some logic but no state)
3. **CheckpointManager** (medium - state but clear interface)
4. **InnerTrainer** (hard - complex logic)
5. **OuterEvaluator** (hard - complex logic)

**After each extraction:**
1. Write tests for new module
2. Update `training_runner.py` to use new module
3. Run tests
4. Commit

---

### Step 4: Verify Equivalence

After refactoring:
```python
def test_training_runner_after_refactor():
    """Verify results identical to pre-refactor."""
    runner = TrainingRunner(cfg, label_fn)
    result = runner.run()
    
    # Load pre-refactor results
    with open("pre_refactor_results.json") as f:
        expected = json.load(f)
    
    # Verify identical (or within floating-point tolerance)
    assert_allclose(result["test_accs"], expected["test_accs"])
    assert_allclose(result["test_macro_f1s"], expected["test_macro_f1s"])
    ...
```

---

## âš ï¸ **COMMON PITFALLS**

### Pitfall 1: Extracting Too Much at Once

**Bad:**
```
One giant commit:
- Extract InnerTrainer
- Extract OuterEvaluator
- Extract CheckpointManager
- Update training_runner.py
```

If something breaks, where's the bug?

**Good:**
```
Commit 1: Extract CheckpointManager + tests
Commit 2: Update training_runner to use CheckpointManager
Commit 3: Extract InnerTrainer + tests
Commit 4: Update training_runner to use InnerTrainer
...
```

If something breaks, easy to bisect!

---

### Pitfall 2: Not Testing Intermediate States

**Bad:**
```python
# Extract module
class InnerTrainer:
    def train(self, ...):
        # 400 lines of code
        ...

# Use it immediately in training_runner.py
```

What if there's a bug in InnerTrainer?

**Good:**
```python
# Extract module
class InnerTrainer:
    def train(self, ...):
        # 400 lines of code
        ...

# TEST IT FIRST
def test_inner_trainer_minimal():
    trainer = InnerTrainer(cfg, obj_computer)
    result = trainer.train(...)
    assert "best_state" in result
    ...

# Then use it
```

---

### Pitfall 3: Breaking Backward Compatibility

**Bad:**
```python
# Remove old method immediately
def _compute_objective_metric(self, ...):
    # DELETED - use obj_computer.compute() instead
    pass
```

What if other scripts call this method?

**Good:**
```python
# Deprecate, don't delete
def _compute_objective_metric(self, ...):
    """DEPRECATED: Use obj_computer.compute() instead."""
    if self.objective_computer:
        return self.objective_computer.compute(...)
    else:
        # Fallback for backward compatibility
        ...
```

Remove in next major version after warning period.

---

### Pitfall 4: Creating Too Many Small Classes

**Bad:**
```python
class LRScheduler:  # 20 lines
class AugScheduler:  # 20 lines
class MixupApplier:  # 20 lines
class TrainingLoop:  # Uses above 3
class ValidationLoop:  # 30 lines
class InnerTrainer:  # Orchestrates above 5
```

**Now you have 6 classes to understand instead of 1 method!**

**Good:**
```python
class InnerTrainer:  # 450 lines
    def train(self, ...):
        ...
    
    def _apply_lr_warmup(self, ...):  # Private helper
    def _apply_aug_warmup(self, ...):  # Private helper
    def _apply_mixup(self, ...):  # Private helper
```

**Keep helpers private unless they're reusable elsewhere.**

---

## ðŸŽ“ **LESSONS FOR YOUR CAREER**

### 1. Code That Works vs. Code That's Maintainable

Many engineers think:
> "If it works, it's good code."

Senior engineers know:
> "If it works AND is maintainable, it's good code."

**Maintainability means:**
- Can you understand it 6 months later?
- Can a new team member understand it?
- Can you modify it without breaking everything?
- Can you test it in isolation?

**This refactoring improved maintainability while keeping functionality identical.**

---

### 2. Refactoring is NOT Rewriting

**Rewriting:** Throw away old code, start fresh
- ðŸš« High risk (might introduce bugs)
- ðŸš« Loses institutional knowledge
- ðŸš« Takes long time
- âœ… Sometimes necessary (if design is fundamentally broken)

**Refactoring:** Improve structure while preserving behavior
- âœ… Low risk (tests ensure equivalence)
- âœ… Incremental (can stop anytime)
- âœ… Fast (extract + test + verify)
- âœ… Usually the right choice

**We refactored, not rewrote. That's why we succeeded.**

---

### 3. Tests Are Documentation

Good tests tell a story:
```python
def test_checkpoint_manager_patience_increases():
    """
    When validation doesn't improve, patience should increase.
    This is the early stopping mechanism.
    """
    manager = CheckpointManager(cfg, obj_computer)
    
    # Epoch 1: Good performance
    manager.update(state, {"val_acc": 80.0, ...})
    assert manager.patience == 0  # â† Explains expected behavior
    
    # Epoch 2: No improvement
    manager.update(state, {"val_acc": 75.0, ...})
    assert manager.patience == 1  # â† Explains early stopping logic
```

**This test teaches:**
- How CheckpointManager works
- What "patience" means
- When early stopping triggers

**Better than comments (tests can't lie).**

---

### 4. Good Code Tells a Story

**Before refactoring:**
```python
def run(self):
    # 1,393 lines of "what"
    # No clear story
    # Just doing things
```

**After refactoring:**
```python
def run(self):
    # Setup
    inner_trainer = InnerTrainer(...)
    outer_evaluator = OuterEvaluator(...)
    
    for fold in outer_folds:
        for inner_fold in inner_folds:
            # Train inner model
            result = inner_trainer.train(...)
            
            # Collect results
            inner_results.append(result)
        
        # Evaluate on test set
        eval_result = outer_evaluator.evaluate(...)
        
        # Compute metrics
        ...
```

**The story:**
1. Create trainers and evaluators
2. For each outer fold:
   - Train K inner models
   - Evaluate on test set using inner models
   - Compute and report metrics

**Clear. Simple. Understandable.**

---

## ðŸ“‹ **CHECKLIST FOR YOUR OWN REFACTORINGS**

### Before You Start

- [ ] Do you have tests for current behavior?
- [ ] Have you identified clear extraction candidates?
- [ ] Have you planned phases (simple first, complex later)?
- [ ] Do you have stakeholder buy-in (if working on team)?

### During Extraction

- [ ] Write tests for new module FIRST (test-driven)
- [ ] Extract ONE module at a time
- [ ] Keep commits small and focused
- [ ] Run tests after each change
- [ ] Document why you're extracting (not just what)

### After Extraction

- [ ] Verify behavior is identical (equivalence tests)
- [ ] Check for performance regressions (profiling)
- [ ] Update documentation (README, architecture diagrams)
- [ ] Get code review from colleague
- [ ] Plan removal of deprecated code (after grace period)

---

## ðŸŽ¯ **WHEN TO REFACTOR**

### Good Times to Refactor

âœ… **Before adding new feature**
- Easier to add feature to clean code
- Refactoring pays for itself immediately

âœ… **After bug fix**
- Make sure same bug can't happen again
- Improve testability

âœ… **During code review**
- "This is hard to review" â†’ probably needs refactoring
- Improve clarity for reviewer (and future readers)

âœ… **When onboarding new team member**
- Use confusion as signal for poor design
- Improve understanding for newcomers

### Bad Times to Refactor

ðŸš« **Right before deadline**
- High risk, low reward
- Wait until after release

ðŸš« **When code is working fine and rarely changes**
- Don't fix what isn't broken
- Focus on high-churn code

ðŸš« **When you don't have tests**
- Write tests first
- Then refactor safely

---

## ðŸ† **SUCCESS METRICS**

How do you know refactoring succeeded?

### Quantitative

- âœ… **Line count reduced** (25% reduction for us)
- âœ… **Test coverage increased** (7 new comprehensive tests)
- âœ… **Cyclomatic complexity reduced** (~60% for us)
- âœ… **Performance unchanged** (should be similar Â±5%)

### Qualitative

- âœ… **"I understand this now!"** (colleague reaction)
- âœ… **Faster code reviews** (reviewers can focus on logic, not parsing)
- âœ… **Easier to add features** (clear where to add code)
- âœ… **Fewer bugs** (isolated modules â†’ isolated bugs)

---

## ðŸŒŸ **FINAL WISDOM**

### From Martin Fowler
> "Any fool can write code that a computer can understand.  
> Good programmers write code that humans can understand."

### From Kent Beck
> "Make it work, make it right, make it fast."

We made it work (original code).  
We made it right (this refactoring).  
Now it's ready to make it fast (if needed).

### From Robert C. Martin
> "The only way to go fast is to go well."

Skipping refactoring to "save time" always backfires.  
Taking time to refactor pays dividends forever.

---

## ðŸ“š **RECOMMENDED READING**

1. **"Refactoring" by Martin Fowler**
   - The bible of refactoring
   - Catalog of refactoring patterns
   - Essential reading

2. **"Clean Code" by Robert C. Martin**
   - Principles of good code
   - Naming, functions, classes
   - Read chapter 3 (Functions) and chapter 10 (Classes)

3. **"Working Effectively with Legacy Code" by Michael Feathers**
   - How to refactor without tests
   - How to add tests to untested code
   - Advanced techniques

4. **"Design Patterns" by Gang of Four**
   - Classic patterns for code organization
   - Strategy, Template Method, etc.
   - Understand when to apply

---

## ðŸŽ‰ **YOU CAN DO THIS**

Refactoring is a **skill** that improves with practice.

Your first refactoring will be scary. Your tenth will be routine.

**Key habits:**
1. Always write tests first
2. Refactor in small steps
3. Commit frequently
4. Get code reviews
5. Learn from mistakes

**Remember:**
- Every expert was once a beginner
- Every clean codebase was once messy
- Every refactoring is a learning opportunity

**You just saw a professional-grade refactoring. Now go do your own!**

---

**END OF MENTOR'S GUIDE**

*If you have questions, ask a senior engineer. They've all done this before.*  
*If you found this helpful, pay it forward by mentoring others.*

Good luck! ðŸš€

